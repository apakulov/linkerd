<!DOCTYPE html>
<!-- Template for rendering markdown files as html -->
<html>
<title>linkerd documentation</title>

<a href="/files/docs/index.html">Back to docs index</a>
<xmp style="display:none;">
# linkerd in practise

Actually using linkerd in practice consists of these steps:

1. Picking a service discovery mechanism, and populating it.
1. Choosing a deployment topology.
1. Configuring linkerd
1. Deploying linkerd.

## Picking a service discovery mechanism

TBD

{{<anchor "deployment">}}
## Choosing a deployment topology

Since linkerd instances are stateless and independent, individual instances can
be added and removed without affecting other parts of the system.

There are two common deployment models for linkerd: per-host, and as a
sidecar process.

### Per-host deployment

{{< figure src="/images/diagram-per-host-deployment.png" title="linkerd deployed per host." >}}

In the per-host deployment model, one linkerd instance is deployed per host
(whether physical or virtual), and all application service instances on that
host route RPC traffic through this instance.

This model is useful for deployments that are primarily host-based.  Each
service instance on the host can address its corresponding linkerd instance at
a fixed location (typically, `localhost:4140`), obviating the need for any
significant client-side logic.

Since this model requires high concurrency of linkerd instances, a larger
resource profile is usually appropriate. In this model, the loss of an
individual linkerd instance is equivalent to losing the host itself.

### Sidecar deployment

{{< figure src="/images/diagram-sidecar-deployment.png" title="linkerd deployed as a sidecar process." >}}

In the sidecar deployment model, one linkerd instance is deployed per instance
of every application service. Each service instance routes RPC calls through
its corresponding linkerd instance. (Note that this model assumes there is a
mechanism for matching service instances with their corresponding linkerd
instances such that the latter can be accessed at a known location.)

This model is useful for deployments that are primarily instance- or
container-based, as opposed to host-based. For example, with a Kubernetes
deployment, a linkerd container can be deployed as part of the Kubernetes
"pod", and the service instance can address the linkerd instance as if it were
on the same host, i.e. by connecting to `localhost:4140`.

Since this model requires many instances of linkerd, a smaller resource profile
is usually appropriate. In this model, the loss of an individual linkerd
instance is equivalent to losing the corresponding service instance.

### Example: Deploying to Kubernetes

  First, write a configuration file that specifies your linker setup. Here's an example config which specifies two routers, one which takes external http traffic on 8080 and forwards it to a service running on port 7000, and another which routes internal traffic via the services specified in a disco/ directory. Here, we're using the experimental k8s namer, and we use dtabs which build names in the form `/$/io.buoyant.k8s.endpoints/_NAMESPACE_/_PORTNAME_/_SERVICENAME_`.

```yaml
namers:
- kind: io.l5d.experimental.k8s
  prefix: /ns
  tlsWithoutValidation: true
  authTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token

routers:
- protocol: http
  label: ext
  httpUriInDst: false
  servers:
  - port: 8080
    ip: 0.0.0.0
  baseDtab: |
    /www      => /$/inet/127.1/7000;
    /host     => /$/io.buoyant.http.anyHostPfx/www;
    /method   => /$/io.buoyant.http.anyMethodPfx/host;
    /http/1.1 => /method;
    /status   => /$/io.buoyant.http.status;
- protocol: http
  label: int
  httpUriInDst: true
  servers:
  - port: 4140
    ip: 0.0.0.0
  baseDtab: |
    /iface    => /ns/$NAMESPACE;
    /srv      => /iface/rest;
    /method   => /$/io.buoyant.http.anyMethodPfx/srv;
    /http/1.1 => /method;
    /status   => /$/io.buoyant.http.status;
```

  Deploy to k8s in your preferred fashion. One way to provide the linker process running in a container with a configuration is to use k8s secrets. The Dtabs in your configuration should be wired up specific to the services you would like to route.

  Example of how to use [k8s secrets](http://kubernetes.io/v1.1/docs/user-guide/secrets.html) to provide a configuration:

Load the configuration into a secret, for example:
```yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: linkerd-config
  namespace: $NAMESPACE
type: Opaque
data:
  config.yaml: $BASE_64_ENCODED_SECRET
```

  Example service object:
```yaml
  ---
  kind: Service
  apiVersion: v1
  metadata:
    namespace: $NAMESPACE
    name: $SERVICE_NAME

  spec:
    selector:
      app: $SERVICE_NAME

    clusterIP: None
    ports:
    - name: josh
      port: 8080
      nodePort: 0
```

Mount the secret using volume mount, and point the linker to config.yaml.  Here's a (partial) example rc.yml for the service mentioned before:

```yaml
---
kind: ReplicationController
apiVersion: v1
metadata:
  namespace: $NAMESPACE
  name: $SERVICE_NAME
spec:
  replicas: 1
  selector:
    app: $SERVICE_NAME
  template:
    metadata:
      labels:
        app: $SERVICE_NAME
    spec:
      dnsPolicy: ClusterFirst
      volumes:
      - name: linkerd-config
        secret:
          secretName: "linkerd-config"
      containers:
      - name: service
        ....
      - name: l5d
        image: gcr.io/$GCE_PROJECT/linkerd:2016-01-15-99138f8
        args:
        - "/io.buoyant/linkerd/config/config.yaml"
        - "-log.level=DEBUG"
        - "-io.buoyant.tracing.projectId=$PROJECT_ID"
        - "-io.buoyant.tracing.sampleRate=1"
        - "-io.buoyant.tracing.apiUri=http://app.buoyant.io"
        ports:
        - name: router
          containerPort: 4140
        - name: frontend
          containerPort: 8080
        - name: admin
          containerPort: 9990
        volumeMounts:
        - name: "linkerd-config"
          mountPath: "/io.buoyant/linkerd/config"
          readOnly: true
```

</xmp>

<script src="/files/js/lib/strapdown/strapdown.js"></script>
</html>