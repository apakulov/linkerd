<!DOCTYPE html>
<!-- Template for rendering markdown files as html -->
<html>
<head><title>linkerd documentation</title></head>

<a href="/files/docs/index.html">Back to docs index</a>
<xmp theme="" style="display:none;">
# linkerd 0.0.9 User Guide


Welcome to the linkerd user guide! The goal of this document is to serve as a
comprehensive guide to linkerd's features, configuration, and best practices.

# Introduction

linkerd is an out-of-process network stack designed specifically for
microservices. linkerd handles everything needed to make cross-service RPC
calls **safe**, **reliable**, and **performant**. linkerd works in a variety of
environments, and can be dropped in to existing applications with a minimum of
change.

See the [Introductory Guide]({{% ref "doc/introduction.md" %}}) for more about
linkerd's goals, and our motivations for building it.

# Quickstart

linkerd ships with a basic starting configuration. This configuration has
routing rules that allow for service discovery using the simple [file-based
service discovery mechanism](#file-based-service-discovery).

Under the shipped configuration, linkerd will route HTTP requests from
`localhost:4140` to `localhost:9999`, provided the `Host` header on the HTTP
request is set to "web". It will also route all thrift requests from
`localhost:4141` to `localhost:9998`.

See the [Getting Started Guide]({{% ref "doc/getting-started.md" %}}) for more
about getting linkerd running locally.

# Overview

linkerd has no dependencies beyond the JVM, and is explicitly designed to fit
into existing application deployments. It works with a variety of common RPC
and service discovery mechanisms, exports telemetry metrics in a common,
machine-parsable formats, and has modern operational affordances, including
dashboards, sane defaults, and a scoped configuration surface area.

Applications use linkerd by running linkerd instances in known locations, and
proxying RPC calls through these instances---i.e., rather than connecting to
RPC destinations directly, services connect to their corresponding
linkerd instances and treat these instances as if they were the destination
services. Since linkerd instances are stateless and independent, they can be
deployed alongside application code in a variety of configurations and with a
minimum of coordination.

For example, in the case of HTTP, an application instance might connect to
`localhost:4140`, send a `Host` header of "destination-service", and proceed
with the rest of the HTTP transaction as normal, as if it were connected to
service *destination-service* directly.

{{< figure src="/images/diagram-individual-instance.png" title="linkerd acts as an RPC proxy for application instances." >}}

By deferring the mechanics of making the call to linkerd, application code is
decoupled from:

1. knowledge of the production topology;
2. knowledge of the service discovery mechanism; and
3. load balancing and connection management logic.

Furthermore, multi-service applications benefit from a consistent, global
traffic control mechanism. This is particularly important for polyglot
applications, for which it is very difficult to attain this sort of consistency
via libraries.

{{<anchor "concepts">}}
## Basic concepts

In order to use linkerd, there are a few concepts that are helpful to
understand first.

An **address** is a host and port pair. In linkerd, these are typically joined
with a ":" character. For example, `localhost:1234` and `192.168.1.123:5678`
are both addresses.

A **concrete name** is the name of a set of addresses, typically as represented
by some service discovery mechanism. For example, "users-production-dc2" might
be a concrete name corresponding to the cluster above, and this name might be
stored in DNS or in ZooKeeper. (Note that in this example, the name suggests a
particular environment ("production") and datacenter ("dc2"), but linkerd
treats concrete names as strings and doesn't attempt to parse them.)

A **logical name** is the name of an RPC destination *as specified by the
application at run time*. For example, a "users" service might want to issue a
request to a "sessions" service; in this case, the logical name might be simply
"sessions". In the most general case, the logical name comprises all the
information necessary for routing requests, including protocol-specific
information (e.g. URLs and headers for HTTP requests) or information such as
the port that linkerd was listening on. (Note that in this example, the name
suggests a particular service ("users"), but, as with concrete names, linkerd
treats logical names as strings.)

The process of **routing** is the translation from a logical name into a set of
addresses, by way of a concrete name. For example, in the example above,
linkerd might translate the "users" logical name into the
"users-production-dc2" concrete name by applying routing rules, and look up the
set of addresses for this concrete name in ZooKeeper.  (Note that service
discovery lookup is and must be part of the routing process---see
[Routing](#routing) for more.)

By separating the notion of logical and concrete names, and by formalizing
the notion of the mapping between them as the process of routing, linkerd is
able to express operations that are traditionally very difficult to do with
multi-service architectures---including staging and canarying service
instances, dark traffic, blue-green deploys, and cross-cluster or
cross-datacenter failover---as routing configurations on arbitrary
subsections of traffic.

The basic order of operations that linkerd follows for an RPC call is:

1. Apply routing rules to determine the service's concrete name from the
   logical name given in the call, applying any request-specific routing
   context as necessary. (Learn more about [per-request routing
   rules](#routing-overrides).)
2. Communicate with the corresponding service discovery endpoint to map the
   concrete name to a set of "physical" endpoints that can serve the request.
   (This allows routing rules to express precedence and failover between
   alternate service discovery endpoints.)
3. If the service discovery endpoint is unavailable or has no entries for the
   concrete name, go back to step 1 and continue the routing resolution
   process.
4. Once it has a set of physical endpoints, pick a particular one to handle the
   request, based on load-balancing information, including observed RPC
   latencies and what it knows of outstanding request queue sizes.
5. Issue the request to that endpoint.
6. Based on timeouts and retry policies, go back to step 4 as necessary.
7. Finally, return the request content to the caller.


{{< figure src="/images/diagram-under-the-hood.png" title="linkerd resolves logical names to endpoints through a series of steps." >}}

Of course, this all happens under the hood---as far as your application instance
is concerned, it simply makes a connection to a logical destination and
receives a response!

{{<anchor "dtabs">}}
## Routing

One of linkerd's most powerful features is its ability to **route** RPC calls.
By controlling the mapping between logical and concrete names, and by being
able to alter that mapping for arbitrary groups of requests, linkerd is able to
encode complex topological operations---such as "use this staging instance
instead of the production cluster" or "insert this debug proxy in between these
two services"---in a way that's explicit, formalized and scoped. These changes
in routing configuration can be performed at runtime, making them quick and
reversible, in contrast to deployments.

linkerd's core routing logic is defined by *dtabs*, or delegation tables.
Dtabs provide a simple computational model for routing decisions based on
prefix rewriting. Beyond their expressive power, dtabs have two notable features:

* They can be overridden on a per-request basis.
* They express lookups in service discovery, allowing chaining and failover
  across service discovery mechanisms.

Every router has a "base" dtab, controlling its default routing rules. The
rules for a particular request are this base dtab concatenated with any
per-request rules; since dtabs are evaluated from bottom to top, this
allows routing logic to be overridden on a per-request basis.

(Note. We're working on better documentation here, but for the short term, you
can refer to the
[Finagle dtab documentation](http://twitter.github.io/finagle/guide/Names.html)).

(You can also play around with the dtab playground by browsing to
http://localhost:9990/delegator ---see [Administration](#administration) below.)

## Service discovery

A large part of the complexity inherent in running multi-service applications
stems from service discovery. Unfortunately, as the complexity and scale of the
application increases, service discovery becomes difficult to avoid. linkerd is
explicitly designed to reduce this complexity by:

1. Abstracting away the specifics of the underlying service discovery
   mechanism.
2. Providing upgrade paths that allow you to choose appropriate service
   discovery endpoints.
3. Encouraging best practices based on years of experience using service
   discovery in production systems.

linkerd abstracts over service discovery mechanisms to treat them in a simple,
uniform way: as simple data stores that are able to resolve a *concrete names*
into a set of addresses.

This minimalist interaction, combined with linkerd's routing rules, provides
powerful control while reducing complexity. For example, linkerd is able to use
multiple service discovery endpoints and to express precedence and failover
between them.

One important note for using service discovery with linkerd is that linkerd
treats service discovery information as _advisory_, not as a source of truth.
If an address is removed from service discovery, linkerd may continue to send
traffic to it. Conversely, if an address is added to service discovery, linkerd
may elect to not send traffic to it. This advisory treatment is an explicit
design goal and is the product of many years of operational experience---it
helps protect against failures in service discovery, including the failure of
invalid or missing data in service discovery. (And there are sometimes good
reasons to *not* send traffic to instances, even if they're healthy---see the
section on aperture load balancing below.)

(To stop traffic to an instance, the instance may simply stop accepting requests
at its listed port. linkerd's load balancing algorithm will handle the rest.)

Lookups in service discovery are controlled by dtab rules. Specifically, lookup
in a particular service discovery mechanism is specified by adding a **namer**
path to a delegation rule. Namer paths are of the form `/$/NAMER/PATH`, where
`NAMER` is the signifier of the namer, and `PATH` any parameters for that
namer. For example `/io.l5d.fs` specifies the file-based namer;
and `/io.l5d.serversets/foo/bar` specifies the
ZooKeeper ServerSets namer with ServerSet
`/foo/bar`.

{{<anchor "load-balancing">}}
## Load balancing and connection management

Load balancing is a critical component of any high-scale, horizontally scaled
system. By distributing load intelligently across a set of endpoints---even as
that set changes dynamically---linkerd can reduce tail latencies and increase
reliability.

linkerd has a variety of load-balancing algorithms at its disposal, many of
them extensively tuned and tested over years of high-scale, operational use at
Twitter. Out of the box, linkerd is configured to use a powerful and generally
applicable algorithm called _Power of Two Choices_ (P2C). (See [Further
reading](#further-reading) for more.) P2C load balancing chooses endpoints
based on outstanding request count, scales to very high concurrency, and is
applicable to a wide variety of situations.

Similarly, while linkerd has a variety of timeout and retry policies available,
the default configuration is very straight-forward: unlimited timeouts and no
retries.

In future releases, the choice of load balancing algorithm and timeout/retry
policies will be configurable.

# Further reading

* [Hints for Service Oriented Architectures. Marius Eriksen, 2014.](http://monkey.org/~marius/hints.pdf)
* [The Power of Two Choices in Randomized Load Balancing. Michael Mitzenmacher. 2001. IEEE Trans. Parallel Distrib. Syst. 12, 10 (October 2001), 1094-1104.](http://www.eecs.harvard.edu/~michaelm/postscripts/mythesis.pdf)


</xmp>

<script src="/files/js/lib/strapdown/strapdown.js"></script>
</html>